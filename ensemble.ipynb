{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d4da37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# --- Model Imports ---\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# --- Sklearn Helpers ---\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2323008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a7a3acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eb593a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. Load Data & Define Columns ---\n",
    "# Assuming these files are in the correct path from your previous notebooks\n",
    "train = pd.read_csv('../dataset/train_featured.csv')\n",
    "test = pd.read_csv('../dataset/test_featured.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc396245",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_columns = [col for col in train.columns if col.startswith('BlendProperty')]\n",
    "feature_columns = [col for col in train.columns if col not in target_columns and col != 'ID']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbec21db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = train[feature_columns]\n",
    "y_train = train[target_columns]\n",
    "X_test = test[feature_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1380af0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. Define Evaluation Functions ---\n",
    "def calculate_leaderboard_score(mape_cost):\n",
    "    ref_cost = 2.72\n",
    "    score = max(10, 100 - (90 * mape_cost) / ref_cost)\n",
    "    return score\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    mape = mean_absolute_percentage_error(y_true + epsilon, y_pred)\n",
    "    score = calculate_leaderboard_score(mape)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83bbb53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 3. 10-Fold Cross-Validation for the Ensemble ---\n",
    "\n",
    "N_SPLITS = 5\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "# OOF (Out-of-Fold) predictions for each model to get a robust overall score\n",
    "oof_lgbm = np.zeros(y_train.shape)\n",
    "oof_hgbm = np.zeros(y_train.shape)\n",
    "oof_xgb = np.zeros(y_train.shape)\n",
    "oof_cat = np.zeros(y_train.shape)\n",
    "oof_svr=np.zeros(y_train.shape)\n",
    "ensemble_scores = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f9a74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 10-Fold CV for 4-Model Ensemble...\n",
      "----------------------------------------------------------------------\n",
      "Fold 1/5\n",
      "  - Training XGBoost...\n",
      "  - Training HistGradientRegressor...\n",
      "  - Training LightGBM...\n",
      "  - Training CatBoost...\n",
      "  > Fold 1 Ensemble Score: 21.4593\n",
      "\n",
      "Fold 2/5\n",
      "  - Training XGBoost...\n",
      "  - Training HistGradientRegressor...\n",
      "  - Training LightGBM...\n",
      "  - Training CatBoost...\n",
      "  > Fold 2 Ensemble Score: 67.2434\n",
      "\n",
      "Fold 3/5\n",
      "  - Training XGBoost...\n",
      "  - Training HistGradientRegressor...\n",
      "  - Training LightGBM...\n",
      "  - Training CatBoost...\n",
      "  > Fold 3 Ensemble Score: 46.5714\n",
      "\n",
      "Fold 4/5\n",
      "  - Training XGBoost...\n",
      "  - Training HistGradientRegressor...\n",
      "  - Training LightGBM...\n",
      "  - Training CatBoost...\n",
      "  > Fold 4 Ensemble Score: 67.4117\n",
      "\n",
      "Fold 5/5\n",
      "  - Training XGBoost...\n",
      "  - Training HistGradientRegressor...\n",
      "  - Training LightGBM...\n",
      "  - Training CatBoost...\n",
      "  > Fold 5 Ensemble Score: 71.5241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Starting 10-Fold CV for 4-Model Ensemble...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train)):\n",
    "    print(f\"Fold {fold+1}/{N_SPLITS}\")\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    # --- Train and Predict with each model ---\n",
    "    # Model 3: XGBoost\n",
    "    print(\"  - Training XGBoost...\")\n",
    "    xgb = MultiOutputRegressor(XGBRegressor(random_state=42, objective='reg:absoluteerror',n_estimators=1000, n_jobs=-1))\n",
    "    xgb.fit(X_train_fold, y_train_fold)\n",
    "    preds_xgb = xgb.predict(X_val_fold)\n",
    "    oof_xgb[val_idx] = preds_xgb\n",
    "    # Model 2: HistGradientBoostingRegressor\n",
    "    print(\"  - Training HistGradientRegressor...\")\n",
    "    hgbm = MultiOutputRegressor(HistGradientBoostingRegressor(random_state=42, loss='absolute_error',max_iter=1000,learning_rate=0.05, max_leaf_nodes=31))\n",
    "    hgbm.fit(X_train_fold, y_train_fold)\n",
    "    preds_hgbm = hgbm.predict(X_val_fold)\n",
    "    oof_hgbm[val_idx] = preds_hgbm\n",
    "      # Model 1: LightGBM\n",
    "    print(\"  - Training LightGBM...\")\n",
    "    lgbm = MultiOutputRegressor(LGBMRegressor(random_state=42, objective='mape', n_estimators=1000, verbose=-1, n_jobs=-1))\n",
    "    lgbm.fit(X_train_fold, y_train_fold)\n",
    "    preds_lgbm = lgbm.predict(X_val_fold)\n",
    "    oof_lgbm[val_idx] = preds_lgbm\n",
    "\n",
    "    # Model 4: CatBoost\n",
    "    print(\"  - Training CatBoost...\")\n",
    "    cat = MultiOutputRegressor(CatBoostRegressor(random_state=42, loss_function='MAPE', iterations=1000, verbose=0))\n",
    "    cat.fit(X_train_fold, y_train_fold)\n",
    "    preds_cat = cat.predict(X_val_fold)\n",
    "    oof_cat[val_idx] = preds_cat\n",
    "    # # Model 5: Support Vector Regressor\n",
    "    # svr_pipeline = Pipeline([\n",
    "    # ('scaler', StandardScaler()),\n",
    "    # ('svr', SVR(C=10, gamma='scale', kernel='rbf')) # Tune these values!\n",
    "    # ])\n",
    "    # # Model 6: Random Forest Regressor\n",
    "    \n",
    "    # multi_output_svr = MultiOutputRegressor(svr_pipeline)\n",
    "    # multi_output_svr.fit(X_train_fold,y_train_fold)\n",
    "    # preds_svr=multi_output_svr.predict(X_val_fold)\n",
    "    # oof_svr[val_idx]=preds_svr\n",
    "    # --- Ensemble and Evaluate for the Fold ---\n",
    "    ensemble_preds_fold = (preds_lgbm + preds_hgbm + preds_xgb + preds_cat) / 4.0\n",
    "    score = evaluate_model(y_val_fold, ensemble_preds_fold)\n",
    "    ensemble_scores.append(score)\n",
    "    print(f\"  > Fold {fold+1} Ensemble Score: {score:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "047c270a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "Cross-Validation Complete.\n",
      "Average Ensemble Score across 5 folds: 54.8420\n",
      "Std Dev of Ensemble Scores: 18.8302\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Overall CV Score ---\n",
    "print(\"-\" * 70)\n",
    "print(\"Cross-Validation Complete.\")\n",
    "print(f\"Average Ensemble Score across {N_SPLITS} folds: {np.mean(ensemble_scores):.4f}\")\n",
    "print(f\"Std Dev of Ensemble Scores: {np.std(ensemble_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe045d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Out-of-Fold (OOF) Ensemble Score: 54.8420\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate overall OOF score for a single reliable metric\n",
    "overall_oof_preds = (oof_lgbm + oof_hgbm + oof_xgb + oof_cat) / 4.0\n",
    "overall_oof_score = evaluate_model(y_train, overall_oof_preds)\n",
    "print(f\"\\nOverall Out-of-Fold (OOF) Ensemble Score: {overall_oof_score:.4f}\")\n",
    "print(\"-\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa458f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final models on ALL data...\n",
      "- Training final LightGBM...\n",
      "- Training final HistGradientRegressor...\n",
      "- Training final XGBoost...\n",
      "- Training final CatBoost...\n",
      "All final models trained.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Final Training and Prediction on Full Data ---\n",
    "\n",
    "print(\"Training final models on ALL data...\")\n",
    "\n",
    "# Train each model on the full training data\n",
    "print(\"- Training final LightGBM...\")\n",
    "final_lgbm = MultiOutputRegressor(LGBMRegressor(random_state=42, objective='mape', n_estimators=2000, verbose=-1, n_jobs=-1)).fit(X_train, y_train)\n",
    "\n",
    "print(\"- Training final HistGradientRegressor...\")\n",
    "final_hgbm = MultiOutputRegressor(HistGradientBoostingRegressor(random_state=42, loss='absolute_error', max_iter=2000)).fit(X_train, y_train)\n",
    "\n",
    "print(\"- Training final XGBoost...\")\n",
    "final_xgb = MultiOutputRegressor(XGBRegressor(random_state=42, objective='reg:absoluteerror', n_estimators=2000, n_jobs=-1)).fit(X_train, y_train)\n",
    "\n",
    "print(\"- Training final CatBoost...\")\n",
    "final_cat = MultiOutputRegressor(CatBoostRegressor(random_state=42, loss_function='MAE', iterations=2000, verbose=0)).fit(X_train, y_train)\n",
    "# print(\"--Training SVR\")\n",
    "# final_svr=MultiOutputRegressor(svr_pipeline).fit(X_train, y_train)\n",
    "print(\"All final models trained.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c84460f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Making predictions on the test set...\n",
      "Creating submission file...\n",
      "Submission file created successfully: ../submission/5-model_ensemble_submission.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Make predictions with each final model on the test set ---\n",
    "print(\"\\nMaking predictions on the test set...\")\n",
    "test_preds_lgbm = final_lgbm.predict(X_test)\n",
    "test_preds_hgbm = final_hgbm.predict(X_test)\n",
    "test_preds_xgb = final_xgb.predict(X_test)\n",
    "test_preds_cat = final_cat.predict(X_test)\n",
    "# test_preds_svr= final_svr.predict(X_test)\n",
    "# --- Average the predictions for the final ensemble result ---\n",
    "final_ensemble_predictions = (test_preds_lgbm + test_preds_hgbm + test_preds_xgb + test_preds_cat) / 4.0\n",
    "\n",
    "# --- 5. Create Submission File ---\n",
    "print(\"Creating submission file...\")\n",
    "submission_df = test[['ID']].copy()\n",
    "submission_df[target_columns] = final_ensemble_predictions\n",
    "\n",
    "submission_filename = '../submission/5-model_ensemble_submission.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"Submission file created successfully: {submission_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38453f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47049a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d948f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bf5ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
